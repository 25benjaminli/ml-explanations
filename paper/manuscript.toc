\contentsline {section}{\numberline {1}Introduction}{3}{section.1}%
\contentsline {section}{\numberline {2}Multilayer Perceptron (MLP)}{4}{section.2}%
\contentsline {subsection}{\numberline {2.1}Feedforward Process}{5}{subsection.2.1}%
\contentsline {subsection}{\numberline {2.2}Backpropagation}{6}{subsection.2.2}%
\contentsline {subsection}{\numberline {2.3}Resources}{8}{subsection.2.3}%
\contentsline {section}{\numberline {3}Convolutional Neural Networks}{9}{section.3}%
\contentsline {subsection}{\numberline {3.1}Convolutional Layers}{9}{subsection.3.1}%
\contentsline {subsection}{\numberline {3.2}Pooling Layers}{11}{subsection.3.2}%
\contentsline {subsection}{\numberline {3.3}Resources}{12}{subsection.3.3}%
\contentsline {section}{\numberline {4}Transformers}{13}{section.4}%
\contentsline {subsection}{\numberline {4.1}Embedding}{14}{subsection.4.1}%
\contentsline {subsection}{\numberline {4.2}Transformer Block}{15}{subsection.4.2}%
\contentsline {subsubsection}{\numberline {4.2.1}Attention}{15}{subsubsection.4.2.1}%
\contentsline {subsubsection}{\numberline {4.2.2}MLP Layer}{18}{subsubsection.4.2.2}%
\contentsline {subsection}{\numberline {4.3}Output}{19}{subsection.4.3}%
\contentsline {subsection}{\numberline {4.4}Resources}{19}{subsection.4.4}%
\contentsline {section}{\numberline {5}Training Models}{20}{section.5}%
\contentsline {subsection}{\numberline {5.1}LMs: Reinforcement Learning with Human Feedback (RLHF)}{20}{subsection.5.1}%
\contentsline {subsubsection}{\numberline {5.1.1}Reward Model}{21}{subsubsection.5.1.1}%
\contentsline {subsubsection}{\numberline {5.1.2}KL Divergence Calculation + PPO update}{21}{subsubsection.5.1.2}%
\contentsline {subsection}{\numberline {5.2}LMs: Direct Preference Optimization (DPO)}{22}{subsection.5.2}%
\contentsline {subsection}{\numberline {5.3}Resources}{23}{subsection.5.3}%
